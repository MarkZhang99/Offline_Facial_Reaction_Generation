{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python evaluate.py  --resume ./results/train_offline/best_checkpoint.pth  --gpu-ids 1  --outdir results/val_offline --split test\n",
    "python evaluate.py   --resume ./results/train_offline/best_checkpoint.pth   --gpu-ids 1   --outdir results/val_offline   --split test  --no-render\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_belfusion.py config=config/1_belfusion_vae.yaml name=All_VAEv2_W50\n",
    "python train_belfusion.py config=config/2_belfusion_ldm.yaml name=Belfusion arch.args.k=10 arch.args.online=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) 路径与清理\n",
    "conda deactivate || true\n",
    "conda env remove -p /transfer/conda/envs/react -y 2>/dev/null || true\n",
    "export CONDA_ENVS_PATH=/transfer/conda/envs\n",
    "export CONDA_PKGS_DIRS=/transfer/conda/pkgs\n",
    "mkdir -p \"$CONDA_ENVS_PATH\" \"$CONDA_PKGS_DIRS\"\n",
    "\n",
    "# 1) 新建环境（按你要的 py3.9）\n",
    "conda create -p /transfer/conda/envs/react python=3.9 -y\n",
    "conda activate /transfer/conda/envs/react\n",
    "\n",
    "# 2) 先装“conda 版”的 PyTorch 2.0.1 + CUDA 11.8（关键！别用 pip）\n",
    "conda install -y -c pytorch -c nvidia \\\n",
    "  pytorch=2.0.1 torchvision=0.15.2 torchaudio=2.0.2 pytorch-cuda=11.8\n",
    "\n",
    "# 3) 安装 PyTorch3D 的依赖（从 conda-forge 拿 fvcore 和 iopath）\n",
    "conda install -y -c conda-forge fvcore=0.1.5 iopath=0.1.10\n",
    "\n",
    "# 4) 安装 PyTorch3D 0.7.4（pytorch3d 官方 channel）\n",
    "conda install -y -c pytorch3d pytorch3d=0.7.4\n",
    "\n",
    "# 5) 防止“同名遮蔽”（项目根别有 ./pytorch3d/ 目录）\n",
    "cd /transfer/Master_Project/Offline_Multiple_Appropriate_Facial_Reaction/BeLFusion\n",
    "[ -d pytorch3d ] && mv pytorch3d _pytorch3d_src_ignored\n",
    "\n",
    "# 6) 验证\n",
    "python - <<'PY'\n",
    "import torch, pytorch3d\n",
    "from pytorch3d.structures import Meshes\n",
    "print(\"torch\", torch.__version__, \"cuda\", torch.version.cuda)\n",
    "print(\"pytorch3d from\", pytorch3d.__file__)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1 -> Expert_video\n",
    "find . -depth -type d -name P1 -print0 | while IFS= read -r -d '' d; do\n",
    "  tgt=\"${d%/P1}/Expert_video\"\n",
    "  # 如果目标已存在且不是软链接，就跳过，防止覆盖真实目录\n",
    "  if [ -e \"$tgt\" ] && [ ! -L \"$tgt\" ]; then\n",
    "    echo \"[skip] target exists: $tgt\"\n",
    "    continue\n",
    "  fi\n",
    "  # 若有残留同名软链接，先删\n",
    "  [ -L \"$tgt\" ] && rm -f \"$tgt\"\n",
    "  mv \"$d\" \"$tgt\"\n",
    "  echo \"[renamed] $d -> $tgt\"\n",
    "done\n",
    "\n",
    "# P2 -> Novice_video\n",
    "find . -depth -type d -name P2 -print0 | while IFS= read -r -d '' d; do\n",
    "  tgt=\"${d%/P2}/Novice_video\"\n",
    "  if [ -e \"$tgt\" ] && [ ! -L \"$tgt\" ]; then\n",
    "    echo \"[skip] target exists: $tgt\"\n",
    "    continue\n",
    "  fi\n",
    "  [ -L \"$tgt\" ] && rm -f \"$tgt\"\n",
    "  mv \"$d\" \"$tgt\"\n",
    "  echo \"[renamed] $d -> $tgt\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python - <<'PY'\n",
    "import os, yaml, pandas as pd, cv2, inspect\n",
    "from dataset import ReactionDataset\n",
    "\n",
    "# 1) 找数据根 & 打印前几行\n",
    "cfg = yaml.safe_load(open('results/train_offline/config.yaml'))\n",
    "root = cfg.get('dataset_path', '../dataset/data')   # 这里是 evaluate 用到的 dataset_path\n",
    "csv = os.path.join(root, 'test.csv')       # 你现在用的是 --split test\n",
    "print('root =', root)\n",
    "print('csv  =', csv, 'exists=', os.path.exists(csv))\n",
    "df = pd.read_csv(csv, header=None)\n",
    "print('HEAD:\\n', df.head(5).to_string(index=False))\n",
    "\n",
    "# 2) 找到第一条两边都存在的样本\n",
    "ok_idx = None\n",
    "for i,(sp,lp,*_) in df[[0,1]].itertuples():\n",
    "    sp, lp = str(sp), str(lp)\n",
    "    if os.path.exists(sp) and os.path.exists(lp):\n",
    "        ok_idx = i; break\n",
    "    else:\n",
    "        print('[skip missing]', i, os.path.exists(sp), sp, '|', os.path.exists(lp), lp)\n",
    "\n",
    "if ok_idx is None:\n",
    "    raise SystemExit(\"❌ test.csv 里没有一条两边路径都存在的样本。先修 CSV 或数据路径。\")\n",
    "\n",
    "print('[probe index]', ok_idx)\n",
    "sp, lp = map(str, df.loc[ok_idx, [0,1]].tolist())\n",
    "\n",
    "# 3) 分别试读 speaker / listener 的路径（视频或CSV）\n",
    "def probe_path(p):\n",
    "    p_lower = p.lower()\n",
    "    if p_lower.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "        cap = cv2.VideoCapture(p)\n",
    "        ok, frame = cap.read()\n",
    "        print('VIDEO open:', p, 'ok=', ok, 'shape=' if ok else '', None if not ok else frame.shape)\n",
    "        cap.release()\n",
    "    elif p_lower.endswith('.csv'):\n",
    "        t = pd.read_csv(p, nrows=3)\n",
    "        print('CSV open:', p, 'shape=', t.shape, 'cols=', list(t.columns)[:5])\n",
    "    else:\n",
    "        print('FILE exists:', p, 'size=', os.path.getsize(p))\n",
    "\n",
    "probe_path(sp)\n",
    "probe_path(lp)\n",
    "\n",
    "# 4) 直接构建 Dataset，拉一条样本（不用 DataLoader）\n",
    "sig = inspect.signature(ReactionDataset.__init__)\n",
    "print('ReactionDataset.__init__ signature:\\n', sig)\n",
    "\n",
    "# 猜测最小必需参数（和 evaluate 一致的关键开关）：\n",
    "ds = ReactionDataset(\n",
    "    root, 'test',\n",
    "    img_size=cfg.get('img_size', 224), crop_size=cfg.get('crop_size', 224),\n",
    "    load_audio=False, load_video_s=True, load_video_l=True,\n",
    "    load_emotion_s=True, load_emotion_l=False  # 按你脚本里的 get_dataloader 传参习惯\n",
    ")\n",
    "print('len(ds)=', len(ds))\n",
    "item = ds[ok_idx]\n",
    "print('✅ got item keys:', item.keys() if isinstance(item, dict) else type(item))\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阶段 B：只算指标（从缓存读，超快）\n",
    "/transfer/conda/envs/react/lib/python3.9/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
    "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
    "  warn(h5py_msg)\n",
    "pred: torch.Size([32, 750, 25]) gt: torch.Size([32, 750, 25])\n",
    "/transfer/Master_Project/Offline_Multiple_Appropriate_Facial_Reaction/BeLFusion/metric/FRC.py:27: RuntimeWarning: invalid value encountered in divide\n",
    "  c /= stddev[:, None]\n",
    "/transfer/Master_Project/Offline_Multiple_Appropriate_Facial_Reaction/BeLFusion/metric/FRC.py:28: RuntimeWarning: invalid value encountered in divide\n",
    "  c /= stddev[None, :]\n",
    "FRC(local-KNN)= 0.12711794751420197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -u evaluate.py \\\n",
    "  --resume ./results/train_offline/best_checkpoint.pth \\\n",
    "  --gpu-ids 1 \\\n",
    "  --outdir results/val_offline \\\n",
    "  --split test \\\n",
    "  --dataset-path ../dataset/data \\\n",
    "  --no-render \\\n",
    "  --limit 32 -b 1 -j 0 \\\n",
    "  --save-preds-dir results/preds_test \\\n",
    "  --save-metrics-cache results/cache/test_limit32.npz \\\n",
    "  --skip-neighbor-metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python compute_metrcs.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRC  = 0.137000033561443\n",
    "FRD  = 75.0037361297156\n",
    "FRVar = 0.006847794633358717\n",
    "FRDvs = 0.008508464321494102\n",
    "S-MSE = 0.0\n",
    "TLCC = 49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "root = \"../dataset/data\"\n",
    "src  = os.path.join(root, \"test.csv\")\n",
    "dst  = os.path.join(root, \"test_diverse.csv\")\n",
    "\n",
    "def _str(p):\n",
    "    return str(p).strip() if p is not None and not (isinstance(p,float) and np.isnan(p)) else \"\"\n",
    "\n",
    "def exists_video(rel):\n",
    "    rel = _str(rel)\n",
    "    return bool(rel) and os.path.exists(os.path.join(root, rel))\n",
    "\n",
    "def infer_listener(sp):\n",
    "    sp = _str(sp)\n",
    "    if \"/Expert_video/\" in sp:\n",
    "        lp = sp.replace(\"/Expert_video/\", \"/Novice_video/\")\n",
    "    elif \"/Novice_video/\" in sp:\n",
    "        lp = sp.replace(\"/Novice_video/\", \"/Expert_video/\")\n",
    "    else:\n",
    "        return \"\"\n",
    "    return lp if exists_video(lp) else \"\"\n",
    "\n",
    "def emotion_csv_rel(rel):\n",
    "    rel = _str(rel)\n",
    "    return os.path.join(root, \"test\", \"Emotion\", f\"{rel}.csv\")\n",
    "\n",
    "def has_emotion(rel):\n",
    "    return os.path.exists(emotion_csv_rel(rel))\n",
    "\n",
    "df0 = pd.read_csv(src, header=None)\n",
    "\n",
    "# --- 自动定位列：优先使用第0行里带有 'speaker_path'/'listener_path' 的列 ---\n",
    "sp_col = None\n",
    "lp_col = None\n",
    "for j in range(min(5, df0.shape[1])):  # 最多前5列里找\n",
    "    v = str(df0.iloc[0, j]).lower()\n",
    "    if \"speaker_path\" in v and sp_col is None: sp_col = j\n",
    "    if \"listener_path\" in v and lp_col is None: lp_col = j\n",
    "\n",
    "if sp_col is None or lp_col is None:\n",
    "    # 后备：常见结构是 [index, speaker, listener]\n",
    "    if df0.shape[1] >= 3:\n",
    "        sp_col, lp_col = 1, 2\n",
    "    elif df0.shape[1] == 2:\n",
    "        sp_col, lp_col = 0, 1\n",
    "    else:\n",
    "        raise RuntimeError(f\"CSV列不足，shape={df0.shape}\")\n",
    "\n",
    "# 丢掉表头行，取两列\n",
    "df = df0.iloc[1:, [sp_col, lp_col]].copy()\n",
    "df.columns = [\"speaker_path\", \"listener_path\"]\n",
    "\n",
    "# 统一转字符串，去空白\n",
    "df[\"speaker_path\"] = df[\"speaker_path\"].map(_str)\n",
    "df[\"listener_path\"] = df[\"listener_path\"].map(_str)\n",
    "\n",
    "# 用 speaker 推断缺失的 listener\n",
    "miss = df[\"listener_path\"].eq(\"\") | df[\"listener_path\"].isna()\n",
    "df.loc[miss, \"listener_path\"] = df.loc[miss, \"speaker_path\"].map(infer_listener)\n",
    "\n",
    "# 过滤：两边视频都存在\n",
    "ok = df[\"speaker_path\"].map(exists_video) & df[\"listener_path\"].map(exists_video)\n",
    "df = df[ok].copy()\n",
    "\n",
    "# 再过滤：两边 Emotion CSV 都存在\n",
    "ok2 = df[\"speaker_path\"].map(has_emotion) & df[\"listener_path\"].map(has_emotion)\n",
    "df = df[ok2].copy()\n",
    "\n",
    "# 去重\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 抽样更分散（固定种子），比如 200 条\n",
    "np.random.seed(42)\n",
    "n = min(200, len(df))\n",
    "idx = np.linspace(0, len(df)-1, num=n, dtype=int) if n>0 else np.array([], dtype=int)\n",
    "sub = df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "# 写回：加表头行\n",
    "header = pd.DataFrame([[\"\", \"speaker_path\", \"listener_path\"]])\n",
    "out = pd.concat([header, sub], axis=0)\n",
    "out.to_csv(dst, header=False, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"wrote:\", dst, \"rows(without header)=\", len(sub))\n",
    "print(sub.head(5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
