{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python feature_extraction.py --split train --type video --data-dir ../dataset/data --save-dir ../dataset/data\n",
    "python feature_extraction.py --split train --type audio --data-dir ../dataset/data --save-dir ../dataset/data\n",
    "\n",
    "(react) [s5727214@w11907 val]$cd Video_features/NoXI/\n",
    "(react) [s5727214@w11907 NoXI]$find . -mindepth 2 -type f -name '*.pth' | while read f; do   target=\"$(dirname \"$(dirname \"$f\")\")/$(basename \"$f\")\";   mv \"$f\" \"$target\"; done\n",
    "(react) [s5727214@w11907 NoXI]$find . -type d -empty -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. `train.py`ï¼šå…¥å£è„šæœ¬\n",
    "\n",
    "1. **å‚æ•°è§£æ**\n",
    "\n",
    "   * è§£æäº†æ•°æ®è·¯å¾„ï¼ˆ`--data-dir`ï¼‰ã€æ—¥å¿—è·¯å¾„ã€è®­ç»ƒç»†èŠ‚ï¼ˆbatch sizeã€learning rateã€epoch æ•°ç­‰ï¼‰\n",
    "   * æ”¯æŒ `--test` æ ‡å¿—æ¥åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼\n",
    "\n",
    "2. **æ¨¡å‹ç»„è£…**\n",
    "\n",
    "   * **CognitiveProcessor**ï¼ˆè®¤çŸ¥æ¨¡å—ï¼‰ã€**PercepProcessor**ï¼ˆæ„ŸçŸ¥èåˆæ¨¡å—ï¼‰ã€**LipschitzGraph**ï¼ˆåŠ¨ä½œç”Ÿæˆå›¾ï¼‰ä¸‰æ£µæ ‘\n",
    "   * ç”¨è¿™ä¸‰è€…åˆå§‹åŒ–ä¸€ä¸ª **MHP**ï¼ˆMultiâ€‘Head Processingï¼‰ä¸»æ¨¡å‹ï¼Œç„¶åæ¬åˆ° GPU\n",
    "\n",
    "3. **æ•°æ®å‡†å¤‡**\n",
    "\n",
    "   * è¯» `train.csv`\n",
    "   * ä»ç¬¬â€¯2/3 åˆ—æ‹¿åˆ° â€œspeakerâ€ è·¯å¾„ å’Œ â€œlistenerâ€ è·¯å¾„ï¼Œæ‹¼æˆä¸¤ä»½åˆ—è¡¨\n",
    "   * åŠ è½½ `neighbour_emotion_train.npy`ï¼ˆé‚»å±…æƒ…æ„Ÿæ ‡ç­¾çŸ©é˜µï¼‰\n",
    "   * ç”¨ `ActionData` åŒ…è£…æˆ PyTorch Datasetï¼Œå†ç»™ DataLoader\n",
    "\n",
    "4. **è®­ç»ƒç»†èŠ‚**\n",
    "\n",
    "   * æ†ç»‘ Adam + WarmupMultiStepLR è°ƒåº¦å™¨\n",
    "   * ä¸»å¾ªç¯æœ€å¤šè·‘ 100 ä¸ª epochï¼ˆscript é‡Œæ­»å†™äº†ï¼‰ï¼›æ¯ä¸ª epoch è°ƒ lrï¼Œç„¶åè°ƒç”¨ `Trainer.train`\n",
    "   * æ¯ 5 ä¸ª epoch è‡ªåŠ¨å­˜ä¸€æ¬¡æ¨¡å‹å¿«ç…§\n",
    "\n",
    "5. **æµ‹è¯•åˆ†æ”¯**\n",
    "\n",
    "   * å¦‚æœä¼ äº† `--test`ï¼Œåˆ™ç”¨ç±»ä¼¼æµç¨‹ï¼š\n",
    "\n",
    "     * è½½å…¥ `test.csv`ã€`neighbour_emotion_test.npy`\n",
    "     * å»º `ActionData` + DataLoaderï¼ˆbatch å¤§å°è®¾æˆä¸€æ¬¡èƒ½æ‰«å®Œæ‰€æœ‰æ»‘çª—ï¼‰\n",
    "     * è°ƒç”¨ `Trainer.test`ï¼Œç”Ÿæˆé¢„æµ‹ `.pth` å­˜åˆ° `outputs/`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `trainers.py`ï¼šTrainer ç±»\n",
    "\n",
    "1. **Loss & å‡†å¤‡**\n",
    "\n",
    "   * æ ¹æ® `--loss-name`ã€`--neighbor-pattern`ï¼ˆnearestã€pairã€allï¼‰é€‰ä¸åŒ Lossï¼ˆMSEã€Distributionã€AllThreMseLossâ€¦ï¼‰\n",
    "   * æŠŠâ€œé‚»å±…åˆ—è¡¨â€`neighbors`ï¼ˆspeaker/listener è·¯å¾„Â ï¼‹Â npy æ ‡ç­¾ï¼‰å¡è¿›æ¥\n",
    "\n",
    "2. **`random_select`**\n",
    "\n",
    "   * ç»™å®šä¸€ä¸ª sampleï¼ˆâ€œdtype+site+group+pid+clip+idxâ€ï¼‰ï¼Œåœ¨é‚»å±…çŸ©é˜µé‡Œæ‰¾å“ªäº› listener æ˜¯â€œåˆé€‚çš„â€\n",
    "   * éšæœºæŠ½ â‰¤10 ä¸ª neighbor ç”¨äºè®­ç»ƒï¼Œtest æ—¶æŒ‰é¡ºåºå…¨ç”¨ä¸Š\n",
    "\n",
    "3. **`load_npy`**\n",
    "\n",
    "   * æ ¹æ® neighbor è·¯å¾„æ‹¼å‡ºå®ƒçš„ emotion CSVï¼ˆ.csv å­˜çš„æ˜¯ listener çš„é¢éƒ¨ emotion æ—¶é—´åºåˆ—ï¼‰\n",
    "   * è¯» pd.read\\_csvã€åˆ‡åˆ°ç›¸åŒå¸§é•¿åæ¬åˆ° GPU\n",
    "\n",
    "4. **`_parse_data`**\n",
    "\n",
    "   * æ”¶åˆ° DataLoader çš„ä¸€ä¸ª batchï¼švideo ç‰¹å¾ã€audio ç‰¹å¾ã€targets\n",
    "   * æŠŠ video/audio push åˆ° GPUï¼›æ ¹æ®æ¨¡å¼â€œnearestâ€/â€œallâ€Â å†³å®šæ˜¯ pairwiseÂ è¿˜æ˜¯å…¨è¿æ¥\n",
    "   * æœ€ç»ˆè¿”å› `(v_inputs, a_inputs, all_neighbors, lengths)` å››å…ƒç»„ç»™ `train`/`test` ç”¨\n",
    "\n",
    "5. **`train`**\n",
    "\n",
    "   * æ ‡å‡†çš„ training loopï¼š\n",
    "\n",
    "     * æ‹¿åˆ° inputs + neighbors\n",
    "     * `model(v_inputs, a_inputs, targets, lengths)` â†’ speaker\\_features, listener\\_features, â€¦, loss\\_det\n",
    "     * è®¡ç®— DTW/MSE ä¹‹ç±»çš„ lossï¼Œåå‘ï¼Œstep optimizer\n",
    "     * æ‰“ç‚¹ logï¼ˆtime, data, lossï¼‰\n",
    "     * é‡åˆ°ç¬¬ `train_iters` ä¸ª miniâ€‘batch å°± breakï¼ˆscript é‡Œé™åˆ¶äº†æ¯ epoch ç”¨å¤šå°‘ batchï¼‰\n",
    "\n",
    "6. **`test`**\n",
    "\n",
    "   * æŠŠæ¨¡å‹åˆ‡ evalï¼Œloop over batches\n",
    "   * è°ƒ `model.inverse(...)` åšç”Ÿæˆï¼Œè·‘ `combine_preds` æ‹¼å› 750 å¸§å®Œæ•´ç»“æœ\n",
    "   * ä¿å­˜æˆ `result-0.pth`â€¦`result-9.pth`ï¼ˆSAMPLE\\_NUMS=10ï¼‰\n",
    "\n",
    "7. **è¾…åŠ©å‡½æ•°**\n",
    "\n",
    "   * `combine_preds`ï¼šæŠŠ sliding window è¾“å‡ºå† overlapâ€‘add å›æ•´æ¡è§†é¢‘é•¿åº¦\n",
    "   * `modify_outputs`ï¼šå¯é€‰åœ°æŠŠå‰ 15 å¸§ clamp åˆ° 0/1ï¼Œç”¨äº debug æˆ– ablation\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ å¿«é€Ÿå°ç»“\n",
    "\n",
    "* **`train.py`**ï¼šè¯» args â†’ æ„æ¨¡å‹ â†’ å‡†å¤‡æ•°æ®/é‚»å±…çŸ©é˜µ â†’ Trainer.train/trainers â†’ ä¿å­˜æ¨¡å‹\n",
    "* **`trainers.py`**ï¼šå®ç°äº†æ ·æœ¬é€‰é‚»å±…ã€è½½ emotion labelsã€å®é™…çš„å‰å‘/åå‘ã€lossã€æµ‹è¯•ç”Ÿæˆç­‰æ ¸å¿ƒæ­¥éª¤\n",
    "ğŸ˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python feature_extraction.py --split train --type video --data-dir <data-dir> --save-dir <data-dir>\n",
    "python feature_extraction.py --split train --type audio --data-dir <data-dir> --save-dir <data-dir>\n",
    "python feature_extraction.py --split test --type video --data-dir ../dataset/data --save-dir ../dataset/data\n",
    "python feature_extraction.py --split test --type audio --data-dir ../dataset/data  --save-dir ../dataset/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python evaluation.py --data-dir ../data/react_clean --pred-dir ../data/react_clean/outputs/results split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------Evaluating Metric-----------------  \n",
    "Metric: | FRC: 0.19604 | FRD: 84.12176 | S-MSE: 0.00072 | FRVar: 0.00602 | FRDvs: 0.03359 | TLCC: 41.45533  \n",
    "Latex-friendly --> model_name & 0.20 & 84.12 & 0.0007 & 0.0060 & 0.0336 & - & 41.46 \\\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGNN Evaluation Results Analysis\n",
    "æ•°æ®é›†ä¸­çš„å›ºå®šæ—¶é—´åˆ†å‰²å¼•å…¥äº†å¯èƒ½ä¸è‡ªç„¶å¯¹è¯å•å…ƒä¸åŒ¹é…çš„äººå·¥è¾¹ç•Œã€‚REACT 2023å’Œ2024ä¸­ä½¿ç”¨çš„30ç§’ç‰‡æ®µå¯èƒ½æˆªæ–­è‡ªç„¶ååº”åºåˆ—æˆ–ç»„åˆä¸ç›¸å…³çš„äº¤äº’ç‰‡æ®µï¼Œå½±å“æ¨¡å‹å­¦ä¹ å’Œè¯„ä¼°ã€‚\n",
    "## Key Findings\n",
    "\n",
    "The REGNN evaluation results reveal a **\"conservative yet precise\"** generation pattern with distinct strengths and weaknesses. The model demonstrates exceptional accuracy and synchronization capabilities while significantly underperforming in diversity metrics.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "### **Outstanding Performance**\n",
    "**S-MSE: 0.00072** indicates extremely high accuracy, with generated reactions closely matching ground truth patterns within local time windows. This excellence stems from REGNN's reversible graph neural network architecture, which ensures generated reactions strictly adhere to learned distributions from real facial behavior. **TLCC: 41.46** demonstrates superior temporal synchronization, reflecting the model's ability to maintain appropriate conversational timing through explicit graph-based modeling of facial feature dependencies and multi-dimensional edge feature learning.\n",
    "\n",
    "### **Critical Weaknesses**\n",
    "**FRD: 84.12** reveals severely limited diversity, with multiple predictions from identical inputs showing minimal variation. This stems from overly conservative sampling strategies where the reversible constraints prioritize generation quality at the expense of exploration. **FRVar: 0.00602** confirms low temporal variability within generated sequences, suggesting the model learns overly concentrated distributions with insufficient randomness injection. **FRDvs: 0.03359** indicates poor differentiation between listener reactions and speaker features, potentially reflecting excessive mimicry patterns rather than appropriate responsive behavior.\n",
    "\n",
    "## Core Issues\n",
    "\n",
    "REGNN exhibits a **quality-diversity trade-off imbalance**, prioritizing safety over natural human behavioral richness. The reversible architecture creates a double-edged effect: while ensuring anatomically plausible reactions on the real behavior manifold, it overly constrains the exploration space. The explicit graph structure, though beneficial for maintaining facial feature relationships, appears too rigid for generating the natural variability observed in human conversational reactions.\n",
    "\n",
    "## Implications\n",
    "\n",
    "Compared to other methods, REGNN achieves superior accuracy metrics but significantly underperforms in diversity measures. This suggests the model successfully addresses the technical challenge of appropriate reaction generation but fails to capture the inherent richness of human behavioral responses. Future improvements should focus on enhancing sampling diversity through relaxed constraints, dynamic graph structures, and explicit diversity regularization while preserving the model's excellent accuracy and synchronization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regnn Output -- Composition and Significance of 25-Dimensional Facial Expression Vectors\n",
    "\n",
    "The 25-dimensional facial expression vector provides a comprehensive and scientifically grounded representation of human facial behavior by decomposing complex expressions into quantifiable components. **The first 15 dimensions correspond to Action Units (AUs)** based on the Facial Action Coding System (FACS) developed by Paul Ekman and colleagues. Each AU represents specific facial muscle movements with standardized intensity scales, such as AU12 for lip corner pulling (smiling) and AU4 for brow lowering (frowning). These AUs offer anatomical precision and cross-cultural consistency, enabling objective measurement of facial muscle activations that underlie all human expressions.\n",
    "\n",
    "**The subsequent 8 dimensions encode facial expression probabilities** for the basic emotions: neutral, happiness, sadness, surprise, fear, disgust, anger, and contempt. These categories are rooted in evolutionary psychology and cross-cultural emotion research, representing universally recognized emotional states. Each dimension provides a confidence score (0-1) indicating the likelihood of that particular expression, allowing for nuanced representation of mixed emotions and transitional states between expressions.\n",
    "\n",
    "**The final 2 dimensions capture emotional valence and arousal**, based on the circumplex model of affect from emotion psychology. Valence measures the pleasantness-unpleasantness continuum, while arousal quantifies the activation-deactivation dimension. This two-dimensional emotional space provides a continuous representation that complements the discrete expression categories, offering a more complete characterization of affective states that aligns with physiological and psychological research on human emotion.\n",
    "\n",
    "Together, these 25 dimensions create a structured mathematical space where facial expressions can be objectively measured, compared, and analyzed using standard statistical methods, transforming subjective facial behavior into quantifiable data suitable for computational modeling and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([750, 25])\n",
      "Dtype: torch.float32\n",
      "Min value: -0.3284480571746826\n",
      "Max value: 1.1664934158325195\n",
      "Mean (per-dimension, first 5 dims): [0.3283752202987671, 0.22007928788661957, 0.5471466779708862, 0.8927489519119263, 0.9049168229103088]\n",
      "Std  (per-dimension, first 5 dims): [0.10018729418516159, 0.07739455997943878, 0.1337902992963791, 0.06093745678663254, 0.02995392307639122]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152123/4078537311.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(pth_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ä¿®æ”¹ä¸ºä½ çš„ .pth æ–‡ä»¶è·¯å¾„\n",
    "#pth_path = '../data/react_clean/outputs/results/test/NoXI/002_2016-03-17_Paris/Expert_video/5/result-0.pth'\n",
    "pth_path = '../data/react_clean/outputs/results/test/RECOLA/group-3/P45/1/result-0.pth'\n",
    "# åŠ è½½ .pth æ–‡ä»¶\n",
    "data = torch.load(pth_path, map_location='cpu')\n",
    "\n",
    "# æ‰“å°æ•°æ®ç±»å‹\n",
    "print(f\"Loaded data type: {type(data)}\")\n",
    "\n",
    "if isinstance(data, torch.Tensor):\n",
    "    # å¯¹äº Tensorï¼Œæ‰“å°å½¢çŠ¶ã€æ•°æ®ç±»å‹ä»¥åŠä¸€äº›ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Dtype: {data.dtype}\")\n",
    "    print(f\"Min value: {data.min().item()}\")\n",
    "    print(f\"Max value: {data.max().item()}\")\n",
    "    print(f\"Mean (per-dimension, first 5 dims): {data.mean(dim=0)[:5].tolist()}\")\n",
    "    print(f\"Std  (per-dimension, first 5 dims): {data.std(dim=0)[:5].tolist()}\")\n",
    "elif isinstance(data, dict):\n",
    "    # å¦‚æœæ˜¯å­—å…¸ï¼ŒæŸ¥çœ‹é”®å’Œå€¼çš„ç±»å‹å’Œå½¢çŠ¶\n",
    "    print(f\"Keys in dict: {list(data.keys())}\")\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            print(f\"  {k}: Tensor with shape {v.shape}, dtype {v.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {type(v)}\")\n",
    "else:\n",
    "    # å…¶ä»–ç±»å‹\n",
    "    print(\"Data loaded is neither Tensor nor dict. Here is a repr:\")\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrade to Wav2Vec 2.0  \n",
    "è°ƒæ•´æ„ŸçŸ¥èåˆæ¨¡å—ï¼ˆperceptual.pyï¼‰\n",
    "åŸå…ˆçš„è·¨æ¨¡æ€è¾“å…¥æŠ•å½±ï¼ŒéŸ³é¢‘ä¸€èˆ¬æ˜¯ 128 ç»´çš„ VGGishï¼›ç°åœ¨æ¢æˆ 768 ç»´çš„ wav2vecï¼Œå°±è¦æŠŠç¬¬ä¸€å±‚çš„ Conv1d(in_channels=*, out=...) æˆ–è€…çº¿æ€§å±‚çš„ in_dim æ”¹æˆ 768ã€‚ä¾‹å¦‚åœ¨ PerceptualProcessor é‡Œï¼š\n",
    "\n",
    "class PerceptualProcessor(nn.Module):\n",
    "    def __init__(self, video_dim=768, audio_dim=768, fuse_dim=64, â€¦):\n",
    "        super().__init__()\n",
    "        self.audio_proj = nn.Conv1d(audio_dim, fuse_dim, kernel_size=1)\n",
    "        self.video_proj = nn.Conv1d(video_dim, fuse_dim, kernel_size=1)\n",
    "\n",
    "æˆ‘åœ¨ PercepProcessor.forward é‡ŒåŠ å…¥äº†ï¼š  \n",
    "è‡ªåŠ¨æ—¶åºå¯¹é½ï¼šç”¨ adaptative_avg_pool1d æŠŠä»»æ„é•¿çš„éŸ³é¢‘ç‰¹å¾ T_a ä¸‹é‡‡æ ·åˆ°è§†é¢‘ç‰¹å¾é•¿åº¦ T_vã€‚  \n",
    "æ— ä¾µå…¥æ€§ï¼šå¦‚æœ T_a == T_v åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå…¼å®¹ VGGish åŸç”Ÿè¾“å…¥æˆ–ä½ æ–°æçš„ wav2vecã€‚  \n",
    "è°ƒç”¨ MULTModelï¼šä¿æŒæ ¼å¼ (1, B, T, D) ä¼ å…¥å¤šæ¨¡æ€èåˆã€‚  \n",
    "\n",
    "Fuse video and audio features with temporal alignment.  \n",
    "video_inputs: (B, T_v, C, H, W) or precomputed (B, T_v, D_v)  \n",
    "audio_inputs: (B, T_a, path) or precomputed (B, T_a, D_a)  \n",
    "returns: (B, T_out, fused_dim)  \n",
    "\n",
    "1. åœ¨ datasets.py é‡Œåšâ€œç‰¹å¾å¢å¼ºâ€ï¼ˆFeature Augï¼‰\n",
    "å› ä¸ºä½ ç°åœ¨ç›´æ¥è¯»çš„éƒ½æ˜¯ç¦»çº¿ç®—å¥½çš„ video_featuresï¼ˆSwin è¾“å‡ºï¼‰ï¼Œæ²¡æ³•åœ¨çº¿å¯¹åŸå¸§åšå›¾åƒå˜æ¢ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¯»å®Œç‰¹å¾åï¼ŒåŠ ä¸€ç‚¹Gaussian noise æˆ–è€…éšæœºä¸¢å¸§ï¼ˆdropoutï¼‰ï¼Œæ¨¡æ‹Ÿå¢å¼ºæ•ˆæœï¼š\n",
    "        # ===== ç‰¹å¾çº§æ•°æ®å¢å¼ºï¼šå¯¹ video_features åŠ ç‚¹éšæœºå™ªå£° & dropout =====\n",
    "        # v_inputs: Tensor [num_frames, feat_dim]\n",
    "        if self.aug:  # åªåœ¨è®­ç»ƒæ—¶ä¼ å…¥ augmentation=True\n",
    "            # Gaussian Noise\n",
    "            noise = torch.randn_like(v_inputs) * 0.02  # æ ‡å‡†å·®å¯è°ƒ\n",
    "            v_inputs = v_inputs + noise\n",
    "            # Feature Dropout\n",
    "            v_inputs = F.dropout(v_inputs, p=0.1, training=True)\n",
    "        a_inputs = self.load_audio_pth(dtype_site_group_pid_clip_idx)\n",
    "2. åœ¨ trainers.py é‡ŒåŠ  æ—¶åºå¹³æ»‘æŸå¤±\n",
    "åœ¨ Trainer.train() çš„å†…å±‚ loopï¼Œæ‰¾åˆ°è¿™æ®µè®¡ç®—æ€» loss çš„ä½ç½®ï¼ŒæŠŠä¸‹é¢ # â€“â€“ add smooth loss â€“â€“ è¿™å—æ’è¿›å»å°±è¡Œï¼š\n",
    " \n",
    "-            loss = loss_dtw + (loss_det if self.cal_logdets else 0.) + loss_mid\n",
    "+            # â€”â€” æ–°å¢ï¼šæ—¶åºå¹³æ»‘æŸå¤± â€”â€” #\n",
    "+            # listener_features: [B, num_windows, num_frames, feat_dim] æˆ–è€… [B, T, D]\n",
    "+            # å¦‚æœæ˜¯å››ç»´ï¼Œå…ˆ reshapeåˆ° [B*T, D]\n",
    "+            lf = listener_features\n",
    "+            if lf.dim() == 4:\n",
    "+                # e.g. [B, W, F, D] -> [B*W, F, D]\n",
    "+                B, W, F, D = lf.shape\n",
    "+                lf = lf.view(B*W, F, D)\n",
    "+            # è®¡ç®—ç›¸é‚»å¸§å·®å€¼\n",
    "+            smooth_loss = torch.mean(torch.abs(lf[:,1:,:] - lf[:,:-1,:]))\n",
    "+            lambda_smooth = 0.1  # å¯ä»¥æ ¹æ®éªŒè¯é›†å†è°ƒ\n",
    "+\n",
    "+            loss = loss_dtw \\\n",
    "+                 + (loss_det if self.cal_logdets else 0.) \\\n",
    "+                 + loss_mid \\\n",
    "+                 + lambda_smooth * smooth_loss\n",
    "\n",
    "3. MULTModel.py ï¼ˆè·¨æ¨¡æ€èåˆç½‘ç»œï¼‰\n",
    "è¾“å…¥ï¼æŠ•å½±ç»´åº¦\n",
    "\n",
    "\n",
    "- self.orig_d_a, self.orig_d_v = 768, 768\n",
    "+ self.orig_d_a, self.orig_d_v = 768, 768   # ä¿æŒ W2V 768\n",
    "- self.d_a, self.d_v = 128, 128\n",
    "+ self.d_a, self.d_v = 256, 256   # æŠ•å½±åˆ°æ›´é«˜ç»´ï¼Œç»™ Transformer æ›´å¤šå®¹é‡\n",
    "æ³¨æ„åŠ›å¤´æ•° & å±‚æ•°\n",
    "\n",
    "\n",
    "- self.num_heads = 4\n",
    "+ self.num_heads = 8       # æ›´å¤šå¤´ï¼ŒæŠ“ä½æ›´ä¸°å¯Œçš„äº¤äº’\n",
    "\n",
    "- self.layers = 5\n",
    "+ self.layers = 6          # å†å¤šä¸€å±‚è·¨æ¨¡æ€æ³¨æ„åŠ›\n",
    "dropout æ¯”ä¾‹\n",
    "\n",
    "\n",
    "- self.attn_dropout = 0.1\n",
    "+ self.attn_dropout = 0.2  # æé«˜æ­£åˆ™ï¼Œé˜²è¿‡æ‹Ÿåˆ\n",
    "\n",
    "- self.relu_dropout = 0.1\n",
    "+ self.relu_dropout = 0.2\n",
    "è¾“å‡ºç»´åº¦\n",
    "\n",
    "\n",
    "- output_dim = 64\n",
    "+ output_dim = 128        # èåˆåç‰¹å¾å†åŠ å€ï¼Œç»™åç»­è®¤çŸ¥ç½‘ç»œæ›´å¤šâ€œå¼¹è¯â€\n",
    "\n",
    "4. Motor Processor ä¸­çš„å¯é€† GNNï¼ˆLipschitzGraph.pyï¼‰\n",
    "num_features = 50 > 64\n",
    "\n",
    "5. trainer.py \n",
    "tensorboardX.SummaryWriter# log epoch metrics\n",
    "        self.writer.add_scalar('train/loss_dtw', losses_dtw.avg, epoch)\n",
    "        self.writer.add_scalar('train/loss_mid', losses_mid.avg, epoch)\n",
    "        self.writer.add_scalar('train/loss_det', losses_det.avg, epoch)\n",
    "        self.writer.add_scalar('train/smooth_loss', smooth_loss.item(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------Evaluating Metric-----------------\n",
    "Metric: | FRC: 0.0795 | FRD: 137.79676 | S-MSE: 0.00345 | FRVar: 0.02144 | FRDvs: 0.02151 | TLCC: 44.28660\n",
    "Latex-friendly --> model_name & 0.01 & 375.80 & 0.0035 & 0.0214 & 0.0215 & - & 44.29 \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (react )",
   "language": "python",
   "name": "react"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
